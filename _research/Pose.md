---
layout: page
title: 视觉与情智交互机器人
description: 在数字时代的浪潮中，有一支团队正以不懈的努力和创新的精神，推动着计算机视觉技术的发展，为康养领域带来前所未有的变革——这就是我们数字媒体与智能网络实验室的视觉组。位于科技与人文交汇的前沿阵地，我们致力于将尖端技术转化为温暖人心的服务，让科技之光照亮每一个需要关怀的生命角落。
img: /assets/img/research/Robot.png
importance: 2
category:
---

**【研究方向：视觉与情智交互机器人**】

\- 表情交互：通过深度学习算法模拟人类的情感表达，结合表情生成与感知技术，使机器能够呈现“表情”并理解用户的情绪和意图，从而提供个性化响应，进一步实现更加自然和谐的人机交互体验。。

\- 意图理解：深入探索用户行为模式，准确捕捉并解析个体需求，构建高度个性化的服务系统，助力精准康养。

\- 生理信号检测：运用先进的传感技术和数据分析方法，对心率、血压等生命体征进行非侵入式监测，保障老年人及慢性病患者的安全与健康。

\- 行为识别：结合视频分析与深度学习，识别日常生活中的异常活动，及时预警潜在风险，提升居住环境的安全性。

**【成就与荣誉】**

自成立以来，本实验室已发表多篇高水平论文于国际顶级会议，研究成果获得同行广泛认可。同时，我们积极参与各类竞赛，不久前斩获了“华为杯”第六届中国研究生人工智能创新大赛全国季军，在一众竞争者中拔得头筹，展现了团队卓越的技术实力和创新能力。

**【专利】**

[1] 喻莉, 赵慧娟, 何双江, 杜聪炬, 李振宇, 李丙银, 张鑫祾. 一种基于分层特征对齐的面部动作单元检测模型构建方法[P]. 中国, 发明专利.

[2] 喻莉, 李丙银, 杜聪炬, 赵慧娟, 张鑫祾. 一种图像去雾模型的构建方法及应用[P]. 中国, 发明专利.

[3] 喻莉, 赵慧娟, 何双江, 杜聪炬. 一种面向跨身份一致性的面部运动单元检测模型构建方法[P]. 中国, 发明专利.

[4] 喻莉, 何双江, 赵慧娟. Compound Expression Recognition Method with Few Samples of Multi-Domain Adversarial Learning[P]. 美国, 国际发明专利.

[5] 喻莉, 何双江, 赵慧娟. 一种多域对抗学习的小样本条件下复合表情识别方法[P]. 中国, 发明专利.

[7] 喻莉, 杜聪炬. 基于特征分离表征学习的面部运动单元检测方法及系统[P]. 中国, 发明专利.

[8] 喻莉, 尚子桥. 基于注意力机制的面部AU检测模型建立方法及其应用[P]. 中国, 发明专利.

[9] 喻莉, 杜聪炬. 一种基于对抗学习的半监督面部运动单元检测方法和系统[P]. 中国, 发明专利.

[10] 喻莉, 杜聪炬. 基于自适应补丁学习的面部AU检测模型建立方法及应用[P]. 中国, 发明专利.

[11] 喻莉,杜博阳. 一种基于多曝光生成融合的弱光车辆检测方法[P].湖北省:CN202310410770.2,2023-07-21.

[12] 喻莉, 喻晗. 一种基于人体骨架特征的行为检测方法及存储介质[P].中国, 发明专利.

[13] 喻莉, 喻晗. Action Detection Method Based on Human Skeleton Feature And Storage Medium[P]. 美国, 国际发明专利.

[14] 何双江, 项金桥, 喻莉, 董喆, 方博, 鄢浩, 赵慧娟, 杜聪炬, 尚子桥, 靖娟,徐凤祥. 一种面部AU关键点检测的方法、装置和电子设备[P]. 中国, 发明专利.

[15] 何双江, 项金桥, 董喆, 方博, 鄢浩, 喻莉, 赵慧娟, 喻晗, 徐凤祥, 杜聪炬. 一种面部表情识别的方法、装置和电子设备[P]. 中国, 发明专利.

**【论文】**

[1] Zhao H, He S, Du C, et al. KHFA: Knowledge-driven Hierarchical Feature Alignment framework for Subject-invariant Facial Action Unit Detection[J]. IEEE Transactions on Instrumentation and Measurement, 2024.

[2] Du C, Li Z, Zhao H, et al. Heterogeneous heatmap distillation framework based on unbiased alignment for lightweight human pose estimation[J]. Image and Vision Computing, 2024, 146: 105041.

[3] Li Z, Du C, Zhao H, et al. Offset-based Disentangled Representation for Efficient Human Pose Estimation[C]//2024 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2024: 1-6.

[4] Shang Z, Du C, Li B, et al. MMA-Net: Multi-view mixed attention mechanism for facial action unit detection[J]. Pattern Recognition Letters, 2023, 172: 165-171.

[5] Du B, Du C, Yu L. MEGF-Net: Multi-exposure generation and fusion network for vehicle detection under dim light conditions[J]. Visual Intelligence, 2023, 1(1): 28.

[6] He S, Zhao H, Li Y, et al. The Avatar Facial Expression Reenactment Method in the Metaverse based on Overall-Local Optical-Flow Estimation and Illumination Difference[C]//2023 26th International Conference on Computer Supported Cooperative Work in Design, 2023: 1312-1317.

[7] He S, Zhao H, Li Y, et al. Compound Facial Expression Recognition with Multi-Domain Fusion Expression based on Adversarial Learning[C]//IEEE International Conference on Systems, Man, and Cybernetics, 2022: 688-693.

[8] He S, Zhao H, Li Y, et al. CDRL: Contrastive Disentangled Representation Learning Scheme for Facial Action Unit Detection[C]//IEEE International Conference on Tools with Artificial Intelligence, 2022: 652-659.

[9] Du C, Yan Z, Yu H, et al. Hierarchical associative encoding and decoding for bottom-up human pose estimation[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2022, 33(4): 1762-1775.

[10] Du C, Han Y, Li Y. A Scale-Sensitive Heatmap Representation for Multi-Person Pose Estimation[J]. IET Image Processing, 2022, 16(4): 1194-1207.

[11] Han Y, Du C, Li Y. Scale-Aware Heatmap Representation for Human Pose Estimation[J]. Pattern Recognition Letters, 2022, 154: 1-6.

[12] Chen Y, Yu L, Ota K, et al. Hierarchical posture representation for robust action recognition[J]. IEEE Transactions on Computational Social Systems, 2019, 6(5): 1115-1125.

[13] Ai W, Xiang S, Yu L, et al. Robust depth estimation for multi-occlusion in light-field images[J]. Optics Express, 2019.

[14] Xiang S, Deng H, Wu J, et al. Hybrid one-shot depth measuring for stereo-view structured light systems[C]//IEEE Visual Communications and Image Processing, 2018.

[15] Xie C, Yu L, Wang S. Deep feature extraction and multi-feature fusion for similar hand gesture recognition[C]//IEEE Visual Communications and Image Processing, 2018.

[16] Deng H, Wu J, Zhu L, et al. Texture edge-guided depth recovery for structured light-based depth sensor[J]. Multimedia Tools and Applications, 2017.

[17] Xiang S, Yu L, Chen C, et al. No-reference depth assessment based on edge misalignment errors for T+D images[J]. IEEE Transactions on Image Processing, 2015.

[18] Yang Y, Wang X, Liu Q, et al. Dense depth image synthesis via energy minimization for three-dimensional video[J]. Signal Processing, 2015.

[19] Xiang S, Deng H, Wu J, et al. Exemplar-based depth inpainting with arbitrary-shape patches and cross-modal matching[J]. Signal Processing: Image Communication, 2019.

[20] Xiang S, Deng H, Wu J, et al. Interfered depth map recovery with texture guidance for multiple structured light depth cameras[J]. Signal Processing: Image Communication, 2015.

[21] Yang Y, Deng H, Wu J, et al. Depth map reconstruction and rectification through coding parameters for mobile 3D video system[J]. Neurocomputing, 2015.

[22] Zhu R, Yu S, Xu X, et al. Dynamic guidance for depth map restoration[C]//IEEE International Workshop on Multimedia Signal Processing, 2019.

[23] Chen Y, Yu L, Wang S. An improved MRF model for robust color guided depth up-sampling[C]//IEEE Visual Communications and Image Processing, 2017.

[24] He S, Zhao H, Li Y, et al. Trusted Healthcare Smart Brain: Innovational Internet Architectures of Intelligent Collaboration of Multi-institution for the Healthcare Service[C]//IEEE International Conference on Computer Supported Cooperative Work in Design, 2022: 605-610.

[25] He S, Zhao H, Li Y, et al. Optical Flow Fusion Synthesis Based on Adversarial Learning from Videos for Facial Action Unit Detection[C]//The International Conference on Image, Vision and Intelligent Systems, 2022: 561-571.

[26] Xiang S, Yu L, Chen C, et al. No-reference depth assessment based on edge misalignment errors for T+D images[J]. IEEE Transactions on Image Processing, 2015.

[27] Xie C, Yu L, Wang S. Deep feature extraction and multi-feature fusion for similar hand gesture recognition[C]//IEEE Visual Communications and Image Processing, 2018.

[28] Yang Y, Deng H, Wu J, et al. Depth map reconstruction and rectification through coding parameters for mobile 3D video system[J]. Neurocomputing, 2015.

[29] Zhu R, Yu S, Xu X, et al. Dynamic guidance for depth map restoration[C]//IEEE International Workshop on Multimedia Signal Processing, 2019.

[30] Chen Y, Yu L, Wang S. An improved MRF model for robust color guided depth up-sampling[C]//IEEE Visual Communications and Image Processing, 2017.

**【未来展望】**

面对老龄化社会带来的挑战，我们将继续深化在计算机视觉领域的研究，不断拓展技术边界，力求为康养行业提供更多智能化解决方案。我们坚信，通过不懈努力，能够构建一个更加美好、健康、充满爱的世界。

视觉组不仅是科研成果的孵化器，更是梦想与希望的摇篮。在这里，每一份创意都可能成为改变生活的起点，每一次突破都在书写着未来的篇章。欢迎您加入我们，一起见证科技如何点亮生命的光辉！
